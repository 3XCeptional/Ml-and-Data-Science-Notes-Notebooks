### Supervised Machine Learning Algorithms

Supervised learning involves training a model on labeled data, where the input data and corresponding output labels are provided. The algorithm learns to map inputs to outputs and makes predictions on unseen data. Common supervised algorithms include:

1. **Linear Regression**
2. **Logistic Regression**
3. **Support Vector Machines (SVM)**
4. **Decision Trees**
5. **Random Forests**
6. **K-Nearest Neighbors (KNN)**
7. **Naive Bayes**
8. **Gradient Boosting Machines (GBM)**
   - **XGBoost**
   - **LightGBM**
   - **CatBoost**
9. **Artificial Neural Networks (ANN)**
10. **Ridge Regression**
11. **Lasso Regression**

### Unsupervised Machine Learning Algorithms

Unsupervised learning involves training a model on data without explicit labels. The goal is to find patterns or structures in the data. Common unsupervised algorithms include:

1. **K-Means Clustering**
2. **Hierarchical Clustering**
3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
4. **Principal Component Analysis (PCA)**
5. **Independent Component Analysis (ICA)**
6. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**
7. **Autoencoders (can be used for unsupervised feature learning)**
8. **Gaussian Mixture Models (GMM)**
9. **Apriori Algorithm (for Association Rule Learning)**
10. **Self-Organizing Maps (SOM)**

### Deep Learning

Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep architectures) to model complex patterns in large datasets. Common deep learning architectures include:

- **Artificial Neural Networks (ANN)**
- **Convolutional Neural Networks (CNN)**
- **Recurrent Neural Networks (RNN)**
  - **Long Short-Term Memory (LSTM)**
  - **Gated Recurrent Units (GRU)**
- **Autoencoders**
- **Deep Belief Networks (DBN)**
- **Generative Adversarial Networks (GANs)**
- **Transformer Networks**

### Common Deep Learning Algorithms

1. **Backpropagation (for training neural networks)**
2. **Stochastic Gradient Descent (SGD)**
3. **Adam Optimizer**
4. **Dropout (regularization technique)**
5. **Batch Normalization**
6. **Softmax (for classification tasks)**
7. **Activation Functions**
   - **ReLU (Rectified Linear Unit)**
   - **Sigmoid**
   - **Tanh**
   - **Leaky ReLU**
